We decided to take the most liberal, machine learning approach and just fine tune a model. We tried a number of neural nets, but settled on fine tuning a pretrained Vision Transformer (VT), specifically this checkpoint: `microsoft_swin-base-patch4-window7-224` which was trained on ImageNet (1k classes). It is a Swin Transformer which is adjacent to the classic ViT architecture but differs in some key aspects such as having a hierarchy of layers. We used two important transformations in training: a random crop of 80-100%, and a random horizontal flip. We randomly set aside 20% of our data as a test set, on which our model has an accuracy of ~99% and a macro F1 of ~0.99.
